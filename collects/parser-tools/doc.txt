_parser tools_
_lex.ss_
A regular expression is one of the following
character                          match the given character
symbol                             match the sequence of chars in the symbol
				   beware of mzscheme converting the 
				   symbol to all lower case
string                             match its sequence of characters
(eof)				   match end of file
(symbol)                           expand the lex abbreviation named symbol
(* re)                             match 0 or more occurances of re
(+ re)                             match 1 or more occurances of re
(? re)                             match 0 or 1 occurance of re
(: re ...1)                        match one of the listed re (alternation)
(@ re ...)                         match each re in succession (concatenation)
(- char char)                      match any character between two (inclusive)
				   (a single character symbol or string can
				    be used as a character here)
(^ char_or_range ...1)             match any character not listed
(the null concatenation `(@) means epsilon)

To use the _lexer generator_ you must (require (lib "lex.ss" "parser-tools")).
This gives you the following syntatic forms:

> (define-lex-abbrev name re) which associates a regular expression with name
to be used in other regular expressions with the (name) form.  The definition 
of name has the same scoping properties as a normal mzscheme macro definition.
In particular a (define-syntax name ...) could interfere.

> (define-lex-abbrevs [name re] ...) defines several lex-abbrevs

> (lex (re action) ...) expands into a function that takes a lex-buf, matches
 the re's against the buffer and returns the result of executing the 
 corresponding action.  Each action is scheme code that can refer to any
 variables that something in the position of the lex definition can.  
 The variables get-start-pos, get-end-pos, get-lexeme and lex-buf are also
 bound in the action (hiding any outside binding).
 > (get-start-pos) returns a position struct for the first character of the 
     match
 > (get-end-pos) returns a position struct for the last character of the match
 > (get-lexeme) returns a string of the match.
 > lex-buf is the lex-buf passed into the function lex generates.

> (define-tokens group-name (token-name ...)) which binds group-name
  to the group of tokens being defined.  For each token-name a function
  (token-token-name expr) is created, which makes a token with name
  token-name and stores the value of expr in it.  The definition of
  group-name has the same scoping properties as a normal mzscheme macro
  definition.  In particular a (define-syntax group-name ...) could
  interfere.  error cannot be defined as a token since it has special
  use in the parser.
> (define-empty-tokens group-name (token-name ...)) like
  define-tokens, except the token constructor takes no arguments and
  stores the value #f into the token.


Several values are also provided:
> (make-lex-buf input-port) creates a lex-buf from an input-port
> (get-position lex-buf) returns a position struct for the next character in
   the lex buf
> (position-offset position) extracts the position of the character in the
   input stream (1 for 1st character, 10 for 10th character, etc.)
> (position-line position) extracts the line number of the character
> (position-col position) extracts the position of the character in the 
    current line
> (position? x) the question associated with a position struct

Each time the scheme code for a lexer is compiled (e.g. when a .ss
file containing a (lex ...) is loaded/required) the lexer generator is
run.  To avoid this overhead place the lexer into a module and compile
the module to a .zo with 'mzc -z filename' Then place the resulting
.zo file in a subdirectory named 'compiled'


_yacc.ss_

To use the _parser generator_ (require (lib "yacc.ss" "parser-tools")).
This module provides the following syntactic forms:


> (parser args ...) where the possible args may come in any order (as
  long as there are no duplicates and all non-optional arguments are
  present) and are:

> (debug filename) OPTIONAL causes the parser to write the LALR table
    to the file named filename (unless the file exists).  filename
    must be a string.

> (error expression) expression should evaluate to a thunk which will
  be executed for its side-effect whenever the parser encounters an
  error.

> (tokens group-name ...) declares that all of the tokens defined in
  the groups can be handled by this parser.

> (start non-terminal-name) declares the starting non-terminal of the
  grammar.  If it is not present the non-terminal of the first
  production in the grammar is used.

> (end token-name ...) specifies a set of tokens from which some
  member must follow any valid parse.  For example an EOF token would
  be specified for a parser that parses entire files and a NEWLINE
  token for a parser that parses entire lines individually.

> (precs ((assoc token-name ...) ...)) OPTIONAL precedence
  declarations to resolve shift/reduce and reduce/reduce conflicts as
  in YACC/BISON.  assoc must be one of left, right or nonassoc.
  States with multiple shift/reduce or reduce/reduce conflicts or some
  combination thereof are not resolved with precedence.

> (grammar (non-terminal ((grammar-symbol ...) (prec token-name) expression)
                          ...) 
            ...)
  declares the grammar to be parsed.  Each grammar-symbol must be a
  token-name or non-terminal.  The prec declaration is optional.
  expression is an expression which will be evaluated when the input
  is found to match its corresponding production.  The identifiers $1,
  ..., $n are bound in the expression and may hide outside bindings of
  $1, ... $n.  Here n is the number of grammar-symbols on the right of
  the production.  All of the productions for a given non-terminal
  must be grouped with it, i.e. No non-terminal may appear twice on
  the left hand side in a parser.

The result of a parser expression is a function, f, that takes one
argument.  This argument must be a zero argument function, t,
that produces successive tokens of the input each time it is called.
If desired, the t may return symbols instead of tokens.  The parser
will treat symbols as tokens of the corresponding name (with #f as a
value, so it is usual to return symbols only in the case of empty
tokens).  f returns the value associated with the parse tree by the
semantic actions.

Each time the scheme code for a parser is compiled (e.g. when a .ss
file containing a (parser ...) is loaded/required) the parser
generator is run.  To avoid this overhead place the parser into a
module and compile the module to a .zo with 'mzc -z filename' Then
place the resulting .zo file in a subdirectory named 'compiled'.


As example of both lex and yacc here is a read function that works for
a subset of the R5RS scheme syntax: (and must read an entire file)


#cs
(module simple-read mzscheme
  
  (require (lib "lex.ss" "parser-tools")
	   (lib "yacc.ss" "parser-tools"))
  
  (provide simple-read)
  
  (define-tokens value-tokens (BOOL CHARACTER SYM STRNG))
  (define-empty-tokens grouping-tokens (OPEN CLOSE HASHOPEN IMPROPER EOF))
  
  (define read-lexer
    (lex
     [(: #\newline #\space #\tab (comment)) 
      (read-lexer lex-buf)]
     ["#t" 
      (token-BOOL #t)]
     ["#f" 
      (token-BOOL #f)]
     [(@ "#\\" (- #\000 #\377))
      (token-CHARACTER (caddr (string->list (get-lexeme))))]
     ["#\\space" 
      (token-CHARACTER #\space)]
     ["#\\newline" 
      (token-CHARACTER #\newline)]
     [(: (@ (initial) (* (subsequent))) + - "...")
      (token-SYM (string->symbol (get-lexeme)))]
     [#\" 
      (token-STRNG (list->string (get-string-token lex-buf)))]
     [#\(
      'OPEN]
     [#\)
      'CLOSE]
     ["#("
      'HASHOPEN]
     ["."
      'IMPROPER]
     [(eof)
      'EOF]))
  
  (define get-string-token
    (lex
     [(^ #\" #\\) (cons (car (string->list (get-lexeme)))
			(get-string-token lex-buf))]
     [(@ #\\ #\\) (cons #\\ (get-string-token lex-buf))]
     [(@ #\\ #\") (cons #\" (get-string-token lex-buf))]
     [#\" null]))
  
  
  (define-lex-abbrevs
   [initial (: (- a z) (- #\A #\Z) ! $ % & * / : < = > ? ^ _ ~)]
   [subsequent (: (initial) (digit) + - #\. @)]
   [digit (- #\0 #\9)]
   [comment (@ #\; (^ #\newline) #\newline)])
  
  
  (define read-parser
    (parser 
     (start sexp-list)
     (debug "read-table")
     (error (lambda () (error 'read-error)))
     (tokens value-tokens grouping-tokens)
     (end EOF)
     (grammar
      (sexp [(SYM) 3]
            [(OPEN sexp-list CLOSE) 5])
      (sexp-list [() 6]
                 [(sexp sexp-list) 7]))))
  
  (define (simple-read input-port)
    (let ((ip (make-lex-buf input-port)))
      (read-parser (lambda () (read-lexer ip))))))



TODO:
lexer:
Default lex abbrevs

parser:
Document/improve parser error handling.

both:
debugging


Potential future additions:
lexer:
POSIX style regular expressions
User definition of lex operators

parser:
Increase speed of parser generator
Fisher-Burke error recovery
Possibly change the $1 style to something like SML/NJ YACC uses
LALR table compression
Error checking for useless productions
Error check for use of an end token in a production
Error check the use of error productions

both:
Automatic position information handling between the lexer and parser
Code documentation
Increase runtime speed.
