_parser tools_ 

This documentation assumes familiarity with lex and yacc style lexer
and parser generators.

_lex.ss_
A regular expression is one of the following
character                          match the given character
symbol                             match the sequence of chars in the symbol
				   beware of mzscheme converting the 
				   symbol to all lower case
string                             match its sequence of characters
(eof)				   match end of file
(symbol)                           expand the lex abbreviation whose name 
				   is symbol
(* re)                             match 0 or more occurrences of re
(+ re)                             match 1 or more occurrences of re
(? re)                             match 0 or 1 occurrence of re
(: re ...1)                        match one of the listed re (alternation)
(@ re ...)                         match each re in succession (concatenation)
(- char char)                      match any character between two (inclusive)
				   (a single character symbol or string can
				    be used as a character here)
(^ char_or_range ...1)             match any character not listed
(the null concatenation `(@) means epsilon)

To use the _lexer generator_ you must (require (lib "lex.ss" "parser-tools")).
This gives you the following syntactic forms:

> (define-lex-abbrev name re) which associates a regular expression
   with a name to be used in other regular expressions with the
   (symbol) form.  The definition of name has the same scoping
   properties as a normal mzscheme macro definition.  In particular a
   (define-syntax name ...) could interfere.

> (define-lex-abbrevs [name re] ...) defines several lex-abbrevs

> (lexer (re action) ...) expands into a function that takes a
   lex-buf, matches the re's against the buffer and returns the result
   of executing the corresponding action.  Each action is scheme code
   that has the same scope as its lexer definition, except that the
   variables get-start-pos, get-end-pos, get-lexeme,
   return-without-pos and lex-buf are also bound in the action (hiding
   any outside binding).  
   > (get-start-pos) returns a position struct for the first character
      matched
   > (get-end-pos) returns a position struct for the character after
      the last character in the match
   > (get-lexeme) returns the matched string
   > lex-buf is the lex-buf being processed (this is useful for
      matching input with multiple lexers, see example below)
   > (return-without-pos x) is a function (continuation) that
     immediately returns the value of x from the lexer.  This useful
     in a src-pos lexer to prevent the lexer from adding source
     information.  For example:
     (define get-token
       (lexer-src-pos
       ...
       ((comment) (get-token lex-buf))
       ...))
     would wrap the source location information for the comment around
     the value of the recursive call.  Using
     ((comment) (return-without-pos (get-token lex-buf))) 
     will cause the value of the recursive call to be returned without
     wrapping position around it.

   The lexer will raise an exception (exn:read) if none of the regular
   expressions match the input.
   Hint: If ((- #\000 #\377) custom-error-behavior) is the last rule,
   then there will always be a match and custom-error-behavior will be
   executed to handle the error situation as desired, only consuming
   the first character from the input buffer.

> (lexer-src-pos (re action) ...) like a lexer, but the return of each
   match is (list action-result start-pos end-pos) instead of simply
   action-result.
   
> (define-tokens group-name (token-name ...)) which binds group-name
   to the group of tokens being defined.  For each token-name, t, a
   function (token-t expr) is created, which constructs a token with
   name t and stores the value of expr in it.  The definition of
   group-name has the same scoping properties as a normal mzscheme
   macro definition.  In particular a (define-syntax group-name ...)
   could interfere.  error cannot be defined as a token since it has
   special use in the parser.

> (define-empty-tokens group-name (token-name ...)) like
   define-tokens, except the resulting token constructors take no
   arguments and store the value #f into the token.

HINT: Token definitions are usually necessary for interoperating with
a generated parser, but may not be the right choice when using the
lexer in other situations.


Several values are also provided:
> (make-lex-buf input-port) creates a lex-buf from an input-port.  A
  lex-buf automatically treats #\return, #\newline and #\return
  #\newline as single #\newline characters.  For keeping source
  location information, the lex-buf treats a #\tab as however many
  spaces (> 0) are needed to bring the column number to a multiple of
  8.
> (make-lex-buf input-port offsets) creates a lex-buf from an
  input-port that starts at the source location provided by offsets
  rather than at 1.  Offsets must be a list of 3 non-negative integers
  whose car is the line number, whose cadr is the column number and
  whose caddr is the total character offset.
> (get-position lex-buf) returns a position structure for the next
  character in the lex-buf
> (position-offset position) the offset of the character in the stream.
> (position-line position) the line number of the character.
> (position-col position) the offset of the character in the current line.
> (position? x) the question associated with a position struct

Each time the scheme code for a lexer is compiled (e.g. when a .ss
file containing a (lex ...) is loaded/required) the lexer generator is
run.  To avoid this overhead place the lexer into a module and compile
the module to a .zo with 'mzc --zo --auto-dir filename'.  This should
create a .zo file in the 'compiled' subdirectory.

See the combined lexer/parser example below.


_yacc.ss_

To use the _parser generator_ (require (lib "yacc.ss" "parser-tools")).
This module provides the following syntactic form:

> (parser args ...) where the possible args may come in any order (as
  long as there are no duplicates and all non-optional arguments are
  present) and are:

  > (debug filename) OPTIONAL causes the parser generator to write the
     LALR table to the file named filename (unless the file exists).
     filename must be a string.

  > (suppress) OPTIONAL causes the parser generator not to report
     shift/reduce or reduce/reduce conflicts.

  > (src-pos) OPTIONAL causes the generated parser to expect input in
     the form (list token position position) instead of simply token.
     Include this option when using the parser with a lexer generated
     with lexer-src-pos.

  > (error expression) expression should evaluate to a function which
     will be executed for its side-effect whenever the parser
     encounters an error.  If the src-pos option is present, the
     function should accept 5 arguments, 
     (lambda (token-ok token-name token-value start-pos end-pos) ...).     
     Otherwise it should accept 3, 
     (lambda (token-ok token-name token-value) ...).
     The first argument will be #f iff the error is that an invalid
     token was received.  The second and third arguments will be the
     name and the value of the token at which the error was detected.
     The fourth and fifth arguments, if present, provide the source
     positions of that token.

  > (tokens group-name ...) declares that all of the tokens defined in
     the groups can be handled by this parser.

  > (start non-terminal-name) declares the starting non-terminal of
     the grammar.

  > (end token-name ...) specifies a set of tokens from which some
     member must follow any valid parse.  For example an EOF token
     would be specified for a parser that parses entire files and a
     NEWLINE token for a parser that parses entire lines individually.

  > (precs ((assoc token-name ...) ...)) OPTIONAL precedence
     declarations to resolve shift/reduce and reduce/reduce conflicts
     as in YACC/BISON.  assoc must be one of left, right or nonassoc.
     States with multiple shift/reduce or reduce/reduce conflicts or
     some combination thereof are not resolved with precedence.

  > (grammar (non-terminal ((grammar-symbol ...) (prec token-name) expression)
                            ...) 
              ...)

     declares the grammar to be parsed.  Each grammar-symbol must be a
     token-name or non-terminal.  The prec declaration is optional.
     expression is a semantic action which will be evaluated when the
     input is found to match its corresponding production.  Each
     action is scheme code that has the same scope as its parser's
     definition, except that the variables $1, ..., $n are bound in the
     expression and may hide outside bindings of $1, ... $n.  $x is
     bound to the result of the action for the $xth grammar symbol on
     the right of the production, if that grammar symbol is a
     non-terminal, or the value stored in the token if the grammar
     symbol is a terminal.  Here n is the number of grammar-symbols on
     the right of the production.  If the src-pos option is present in
     the parser, variables $1-start-pos, ..., $n-start-pos and
     $1-end-pos, ..., $n-end-pos are also available and refer to the
     position structures corresponding to the start and end of the
     corresponding grammar-symbol.  All of the productions for a given
     non-terminal must be grouped with it, i.e. No non-terminal may
     appear twice on the left hand side in a parser.

The result of a parser expression is a function, f, that takes one
argument.  This argument must be a zero argument function, t, that
produces successive tokens of the input each time it is called.  If
desired, the t may return symbols instead of tokens.  The parser will
treat symbols as tokens of the corresponding name (with #f as a value,
so it is usual to return symbols only in the case of empty tokens).  f
returns the value associated with the parse tree by the semantic
actions.  If the parser encounters an error, after invoking the
supplied error function, it will try to use error productions to
continue parsing.  If it cannot, it raises a read error.

Each time the scheme code for a parser is compiled (e.g. when a .ss
file containing a (parser ...) is loaded/required) the parser
generator is run.  To avoid this overhead place the lexer into a
module and compile the module to a .zo with 'mzc --zo --auto-dir
filename'.  This should create a .zo file in the 'compiled'
subdirectory.


As example of both lex and yacc here is a read function that works for
a subset of the R5RS scheme syntax: (and must read an entire file)


#cs
(module simple-read mzscheme
  
  (require (lib "lex.ss" "parser-tools")
	   (lib "yacc.ss" "parser-tools"))
  
  (provide simple-read)
  
  (define-tokens value-tokens (BOOL CHARACTER SYM STRNG))
  (define-empty-tokens grouping-tokens (OPEN CLOSE HASHOPEN IMPROPER EOF))
  
  (define read-lexer
    (lexer
     [(: #\newline #\space #\tab (comment)) 
      (read-lexer lex-buf)]
     ["#t" 
      (token-BOOL #t)]
     ["#f" 
      (token-BOOL #f)]
     [(@ "#\\" (- #\000 #\377))
      (token-CHARACTER (caddr (string->list (get-lexeme))))]
     ["#\\space" 
      (token-CHARACTER #\space)]
     ["#\\newline" 
      (token-CHARACTER #\newline)]
     [(: (@ (initial) (* (subsequent))) + - "...")
      (token-SYM (string->symbol (get-lexeme)))]
     [#\" 
      (token-STRNG (list->string (get-string-token lex-buf)))]
     [#\(
      'OPEN]
     [#\)
      'CLOSE]
     ["#("
      'HASHOPEN]
     ["."
      'IMPROPER]
     [(eof)
      'EOF]))
  
  (define get-string-token
    (lexer
     [(^ #\" #\\) (cons (car (string->list (get-lexeme)))
			(get-string-token lex-buf))]
     [(@ #\\ #\\) (cons #\\ (get-string-token lex-buf))]
     [(@ #\\ #\") (cons #\" (get-string-token lex-buf))]
     [#\" null]))
  
  
  (define-lex-abbrevs
   [initial (: (- a z) (- #\A #\Z) ! $ % & * / : < = > ? ^ _ ~)]
   [subsequent (: (initial) (digit) + - #\. @)]
   [digit (- #\0 #\9)]
   [comment (@ #\; (* (^ #\newline)) #\newline)]) 
  
  (define read-parser
    (parser 
     (start sexp-list)
     (debug "read-table")
     (error (lambda (x) (error 'read-error)))
     (tokens value-tokens grouping-tokens)
     (end EOF)
     (grammar
      (sexp [(SYM) $1]
            [(OPEN sexp-list CLOSE) $2])
      (sexp-list [() null]
                 [(sexp sexp-list) (cons $1 $2)]))))
  
  (define (simple-read input-port)
    (let ((ip (make-lex-buf input-port)))
      (read-parser (lambda () (read-lexer ip))))))



TODO:
lexer:

parser:
Error check that start production is defined

both:
debugging
raise-read-error arguments

Potential future additions:
lexer:
POSIX style regular expressions
User definition of lex operators

parser:
Increase speed of parser generator
Fisher-Burke error recovery
Possibly change the $1 style to something like SML/NJ YACC uses
LALR table compression
Error checking for useless productions
Error check for use of an end token in a production
Error check the use of error productions

both:
Code documentation
Increase runtime speed
